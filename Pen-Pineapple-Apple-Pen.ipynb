{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doodle Classifier\n",
    "This is a single layer Neural Network made with [TensorFlow](https://github.com/tensorflow/tensorflow) that classifies doodles into either pens or pineapples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as pl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting our data\n",
    "Along with this repository are two files that contains all the doodles that we are going to use. However, we also must create the labels that our Neural Network should correctly guess. Those labels are going to be an array with two elements. If the first element is 1, then we say it's a pineapple, otherwise, if the second element is 1, it would be a pen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the images\n",
    "pineapples_images = np.load('pineapple.npy')\n",
    "pens_images = np.load('pen.npy')\n",
    "\n",
    "#Create our pineapple and pen labels\n",
    "pineapples_labels = np.zeros((pineapples_images.shape[0], 2))\n",
    "pineapples_labels[:, 0] = 1;\n",
    "pens_labels = np.zeros((pens_images.shape[0], 2))\n",
    "pens_labels[:, 1] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a good practise to take a look to our data. In this case, we can see that our images are indeed a 728 array. Each value represents a pixel of the doodle image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Pineapples', (125071, 784), (125071, 2))\n",
      "('Pens', (122001, 784), (122001, 2))\n"
     ]
    }
   ],
   "source": [
    "print('Pineapples', pineapples_images.shape, pineapples_labels.shape)\n",
    "print('Pens', pens_images.shape, pens_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our datasets\n",
    "What we are going to do now is to create three datasets each one with one purpose: **training**, **testing** and **validation**. These datasets contains both the pens and pineapples doodles, that's why we concatenate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 80000\n",
    "train_dataset = np.concatenate((pineapples_images[0:TRAIN_SIZE], pens_images[0:TRAIN_SIZE]))\n",
    "train_labels = np.concatenate((pineapples_labels[0:TRAIN_SIZE], pens_labels[0:TRAIN_SIZE]))\n",
    "\n",
    "TEST_SIZE = 20000\n",
    "offset = TRAIN_SIZE\n",
    "test_dataset = np.concatenate((pineapples_images[offset:offset + TEST_SIZE], pens_images[offset:offset + TEST_SIZE]))\n",
    "test_labels = np.concatenate((pineapples_labels[offset:offset + TEST_SIZE], pens_labels[offset:offset + TEST_SIZE]))\n",
    "\n",
    "VALIDATION_SIZE = 10000\n",
    "offset = offset + TEST_SIZE\n",
    "validation_dataset = np.concatenate((pineapples_images[offset:offset + VALIDATION_SIZE], pens_images[offset:offset + VALIDATION_SIZE]))\n",
    "validation_labels = np.concatenate((pineapples_labels[offset:offset + VALIDATION_SIZE], pens_labels[offset:offset + VALIDATION_SIZE]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's see how our datasets look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (160000, 784), (160000, 2))\n",
      "('Test set', (40000, 784), (40000, 2))\n",
      "('Validation set', (20000, 784), (20000, 2))\n"
     ]
    }
   ],
   "source": [
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n",
    "print('Validation set', validation_dataset.shape, validation_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good, but now we have to face a little problem. As our datasets were concatenated, now all pineapples appears first, and that's no good as we want to get batches of both types of doodles at the same! But worry not, we can easily shuffle both arrays with this simple function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_images_and_labels(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now you can shuffle our datasets, you can run this function as many times as you want and see how it's working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pineapple\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f890febf790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset, train_labels = shuffle_images_and_labels(train_dataset, train_labels)\n",
    "test_dataset, test_labels = shuffle_images_and_labels(test_dataset, test_labels)\n",
    "validation_dataset, validation_labels = shuffle_images_and_labels(validation_dataset, validation_labels)\n",
    "\n",
    "sample = test_dataset[1]\n",
    "sample = sample.reshape(28,28)\n",
    "pl.imshow(sample, cmap=\"gray_r\")\n",
    "print(\"Pen\" if (test_labels[1, 0] == 0) else \"Pineapple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Neural Network!\n",
    "Okey, let's do it. So we have a 784 pixels doodle. And also we have 160000 doodles to train our network. So... we could feed one by one to our NN. But that would be too slow. Feeding all of them at the same time would be quite overwhelming and would not let room for optimization, so the best approach is to define a batch size that will be the number of doodles that we are going to feed our Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we know how to give all our data to our network, but we must create it first! To do this, we are going to define several things: how many **inputs** are we going to get each iteration, how many **outputs** will be predicted, the number of **weights and biases** that will change during the training fase, the **loss and optimizer** functions, how our NN will predict its output... This block of code does all of those things in our TensorFlow object graph, that would be our Neural Network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=[BATCH_SIZE, 28*28])\n",
    "    tf_train_labels = tf.placeholder(tf.int64, shape=[BATCH_SIZE, 2])\n",
    "    \n",
    "    weights = tf.Variable(tf.random_uniform([784, 2]))\n",
    "    biases = tf.Variable(tf.random_uniform([2]))\n",
    "    \n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=tf_train_labels))\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    tf_test_dataset = tf.cast(tf.constant(test_dataset), tf.float32)\n",
    "    tf_validation_dataset = tf.cast(tf.constant(validation_dataset), tf.float32)\n",
    "    \n",
    "    train_prediction = tf.nn.softmax((tf.matmul(tf_train_dataset, weights) + biases))\n",
    "    test_prediction = tf.nn.softmax((tf.matmul(tf_test_dataset, weights) + biases))\n",
    "    validation_prediction = tf.nn.softmax((tf.matmul(tf_validation_dataset, weights) + biases))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Place your bets\n",
    "Now let's train our Neural Network. It knows what to do as we already defined its optimization and loss function, but in order to see if it is performing correctly, we are going to create an accuracy function that tells us how many correct predictions our NN does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.equal(np.argmax(predictions, 1), np.argmax(labels, 1))) / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okey, now everything it's ready. We are going to train our NN. To do this, we are going to do 10001 iterations, each one will take a batch of 128 doodles and our NN will try to approach a function that correctly predicts the doodle category. In order to visualize its progress, we'll print it's accuracy every 500 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 10000: 20669.662109\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation error 92.8%\n",
      "Successfully ended iteration 10000\n",
      "Test accuracy: 93.5%\n"
     ]
    }
   ],
   "source": [
    "NUM_STEPS = 10001\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for step in range(NUM_STEPS):\n",
    "        offset = (step * 128) % (train_labels.shape[0] - BATCH_SIZE)\n",
    "        batch_data = train_dataset[offset:(offset + BATCH_SIZE), :]\n",
    "        batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "        feed_dict = { tf_train_dataset: batch_data, tf_train_labels: batch_labels }\n",
    "       \n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        if(step % 500 == 0):\n",
    "            clear_output(wait=True)\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation error %.1f%%\" % accuracy(validation_prediction.eval(), validation_labels))\n",
    "    print(\"Successfully ended iteration %d\" % step)\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
